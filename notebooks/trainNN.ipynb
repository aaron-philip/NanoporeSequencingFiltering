{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c286e47",
   "metadata": {},
   "source": [
    "# Training Networks\n",
    "In this Notebook, we train networks on the HDF5 Database built. Each network takes in the noisy data, and attempts to predict the optimal circuit parameters that correspond. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce720ab6",
   "metadata": {},
   "source": [
    "## Custom DataLoader\n",
    "\n",
    "We need our own dataloader class to extract the correct signals/parameters. We then divide this randomly into training, test, and validation subgroups in an 80:10:10 ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d93f7638",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as func\n",
    "import h5py\n",
    "import os\n",
    "\n",
    "# Custom Dataloader for our NN from .h5 database\n",
    "class HDF5Data(Dataset):\n",
    "    def __init__(self, path_to_h5, ftype):\n",
    "        self.path_to_h5 = path_to_h5\n",
    "        with h5py.File(path_to_h5, 'r') as f:\n",
    "            signaldata = f['NoisySignals'][()]\n",
    "            \n",
    "            ftypeValid = False\n",
    "            if ftype=='lowpass':\n",
    "                ftypeValid = True\n",
    "                params = f['LowPass'][()]\n",
    "            elif ftype=='highpass':\n",
    "                ftypeValid = True\n",
    "                params = f['HighPass'][()]\n",
    "            elif ftype=='bandpass':\n",
    "                ftypeValid = True\n",
    "                params = f['BandPass'][()]\n",
    "            elif ftype=='butterlowpass':\n",
    "                ftypeValid = True\n",
    "                params = f['ButterworthLowPass'][()]\n",
    "            \n",
    "            if ftypeValid==False:\n",
    "                raise ValueError(\"Given filter name is not one of the options\")\n",
    "            datalen = len(signaldata)\n",
    "            \n",
    "        self.signaldata = signaldata\n",
    "        self.params = params\n",
    "        self.datalen = datalen\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.datalen\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.signaldata[idx], self.params[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "537d2f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splits full dataset into training, test, and validation sets\n",
    "def loaderSplit(path_to_h5, ftype):\n",
    "    dataset = HDF5Data(PATH_TO_H5, ftype)\n",
    "\n",
    "    # Split --> training:test = 80:20\n",
    "    train_set_size = int(len(dataset) * 0.8)\n",
    "    test_set_size = len(dataset) - train_set_size\n",
    "    train_set, test_set = torch.utils.data.random_split(dataset, [train_set_size, test_set_size])\n",
    "\n",
    "    # Split test --> test:valid = 10:10\n",
    "    test_set_size = int(len(test_set)*0.5)\n",
    "    valid_set_size = len(test_set) - test_set_size\n",
    "    valid_set, test_set = torch.utils.data.random_split(test_set, [valid_set_size, test_set_size])\n",
    "\n",
    "    # Final Split --> training:test:valid = 80:10:10 \n",
    "    print(\"Data Points in Training Set:\", len(train_set))\n",
    "    print(\"Data Points in Test Set:\",len(test_set))\n",
    "    print(\"Data Points in Validation Set:\",len(valid_set))\n",
    "\n",
    "    # Using PyTorch DataLoader\n",
    "    test_loader = DataLoader(test_set, batch_size=64, shuffle=True)\n",
    "    train_loader = DataLoader(train_set, batch_size=64, shuffle=True)\n",
    "    valid_loader = DataLoader(valid_set, batch_size=64, shuffle=True)\n",
    "    \n",
    "    return test_loader, train_loader, valid_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3a60e5",
   "metadata": {},
   "source": [
    "## Deep Models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "27c3b330",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General Deep Network, 4 hiddlen layers, 1 Relu activation\n",
    "class NN(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(NN, self).__init__()\n",
    "        self.l1 = nn.Linear(input_size, 32)\n",
    "        self.l2 = nn.Linear(32, 24)\n",
    "        self.l3 = nn.Linear(24, 32)\n",
    "        self.l4 = nn.Linear(32, 8)\n",
    "        self.l5 = nn.Linear(8, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = func.relu(self.l1(x))\n",
    "        x = self.l2(x)\n",
    "        x = self.l3(x)\n",
    "        x = self.l4(x)\n",
    "        x = self.l5(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2fbb904e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handles training for one epoch\n",
    "def train_epoch(epoch_idx, loader, model):\n",
    "    running_loss = 0\n",
    "    last_loss = 0\n",
    "    \n",
    "    for batch_idx, data in enumerate(loader):\n",
    "        # Every data instance is an input + label pair\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Zero your gradients for every batch!\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Make predictions for this batch\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Compute the loss and its gradients\n",
    "        loss = mse_loss(outputs, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        # Adjust learning weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Gather data and report\n",
    "        running_loss += loss.item()\n",
    "        if batch_idx % 64 == 63:\n",
    "            last_loss = running_loss / 64 # loss per batch\n",
    "            print('  batch {} loss: {}'.format(batch_idx+1, last_loss))\n",
    "            running_loss = 0\n",
    "    return last_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2989e8ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Points in Training Set: 7200\n",
      "Data Points in Test Set: 900\n",
      "Data Points in Validation Set: 900\n"
     ]
    }
   ],
   "source": [
    "PATH_TO_H5 = '/Users/aaronphilip/ScienceFair/projects/NanoporeSequencingFiltering/database/NanoporeFiltered.h5'\n",
    "ftype = 'lowpass'\n",
    "test_loader, train_loader, valid_loader = loaderSplit(PATH_TO_H5, ftype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8be549ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "50bb6480",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 100 long input vector --> 2 long output vector \n",
    "lowModel = NN(100, 2).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2e520523",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "epochs = 20\n",
    "learning_rate = 0.01\n",
    "\n",
    "# Optimizer and Loss Function\n",
    "optimizer = optim.Adam(lowModel.parameters(), lr=learning_rate)\n",
    "mse_loss = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b6b43120",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "  batch 64 loss: 0.20420456200372428\n",
      "Loss  train 0.20420456200372428 Validation 0.19586306810379028\n",
      "Epoch: 2\n",
      "  batch 64 loss: 0.19739924801979214\n",
      "Loss  train 0.19739924801979214 Validation 0.19454459846019745\n",
      "Epoch: 3\n",
      "  batch 64 loss: 0.20406357652973384\n",
      "Loss  train 0.20406357652973384 Validation 0.2502115070819855\n",
      "Epoch: 4\n",
      "  batch 64 loss: 0.19689722266048193\n",
      "Loss  train 0.19689722266048193 Validation 0.19213297963142395\n",
      "Epoch: 5\n",
      "  batch 64 loss: 0.20220009190961719\n",
      "Loss  train 0.20220009190961719 Validation 0.21825146675109863\n",
      "Epoch: 6\n",
      "  batch 64 loss: 0.19707565032877028\n",
      "Loss  train 0.19707565032877028 Validation 0.1933043897151947\n",
      "Epoch: 7\n",
      "  batch 64 loss: 0.19391681323759258\n",
      "Loss  train 0.19391681323759258 Validation 0.19299575686454773\n",
      "Epoch: 8\n",
      "  batch 64 loss: 0.1931652156636119\n",
      "Loss  train 0.1931652156636119 Validation 0.1965872347354889\n",
      "Epoch: 9\n",
      "  batch 64 loss: 0.19960960082244128\n",
      "Loss  train 0.19960960082244128 Validation 0.19304046034812927\n",
      "Epoch: 10\n",
      "  batch 64 loss: 0.20223672199063003\n",
      "Loss  train 0.20223672199063003 Validation 0.19454875588417053\n",
      "Epoch: 11\n",
      "  batch 64 loss: 0.19800518604461104\n",
      "Loss  train 0.19800518604461104 Validation 0.1997770369052887\n",
      "Epoch: 12\n",
      "  batch 64 loss: 0.19100951682776213\n",
      "Loss  train 0.19100951682776213 Validation 0.19785849750041962\n",
      "Epoch: 13\n",
      "  batch 64 loss: 0.19539150700438768\n",
      "Loss  train 0.19539150700438768 Validation 0.19181592762470245\n",
      "Epoch: 14\n",
      "  batch 64 loss: 0.19489858904853463\n",
      "Loss  train 0.19489858904853463 Validation 0.19456608593463898\n",
      "Epoch: 15\n",
      "  batch 64 loss: 0.1842779174912721\n",
      "Loss  train 0.1842779174912721 Validation 0.20380841195583344\n",
      "Epoch: 16\n",
      "  batch 64 loss: 0.20213129720650613\n",
      "Loss  train 0.20213129720650613 Validation 0.19255608320236206\n",
      "Epoch: 17\n",
      "  batch 64 loss: 0.19863946142140776\n",
      "Loss  train 0.19863946142140776 Validation 0.1904284507036209\n",
      "Epoch: 18\n",
      "  batch 64 loss: 0.20648517971858382\n",
      "Loss  train 0.20648517971858382 Validation 0.19379279017448425\n",
      "Epoch: 19\n",
      "  batch 64 loss: 0.19394650834146887\n",
      "Loss  train 0.19394650834146887 Validation 0.21674038469791412\n",
      "Epoch: 20\n",
      "  batch 64 loss: 0.18977228400763124\n",
      "Loss  train 0.18977228400763124 Validation 0.19126221537590027\n",
      "tensor(0.1904, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "best_vloss = 1_000_000\n",
    "for epoch in range(epochs):\n",
    "    print('Epoch: %s' % (epoch+1))\n",
    "\n",
    "    # Make sure gradient tracking is on, and do a pass over the data\n",
    "    lowModel.train(True)\n",
    "    avg_loss = train_epoch(epoch, train_loader, lowModel)\n",
    "\n",
    "    # We don't need gradients on to do reporting\n",
    "    lowModel.train(False)\n",
    "\n",
    "    running_vloss = 0.0\n",
    "    for i, vdata in enumerate(valid_loader):\n",
    "        vinputs, vlabels = vdata\n",
    "        voutputs = lowModel(vinputs)\n",
    "        vloss = mse_loss(voutputs, vlabels)\n",
    "        running_vloss += vloss\n",
    "\n",
    "    avg_vloss = running_vloss / (i + 1)\n",
    "    print('Loss  train {} Validation {}'.format(avg_loss, avg_vloss))\n",
    "\n",
    "    # Track best performance, and save the model's state\n",
    "    if avg_vloss < best_vloss:\n",
    "        best_vloss = avg_vloss\n",
    "\n",
    "        \n",
    "print(best_vloss)\n",
    "model_path = '../models/low_model.pt'\n",
    "torch.save(lowModel.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf499e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a71ad2b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
